import os
from langchain_community.document_loaders import WebBaseLoader # 1. ë¡œë“œ
from langchain_text_splitters import RecursiveCharacterTextSplitter # 2. ë¶„í• 
from langchain_community.vectorstores import FAISS # 3. ì €ì¥ (Vector DB - ë¡œì»¬ ë°±ì—…ìš©)
from langchain_postgres import PGVector # 3. ì €ì¥ (Vector DB - PostgreSQL)
from langchain_huggingface import HuggingFaceEmbeddings # 3. ì„ë² ë”© (HF ëª¨ë¸ 1)
from langchain_huggingface import HuggingFacePipeline # 5. ìƒì„± (HF ëª¨ë¸ 2)
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from sqlalchemy import create_engine, text
import psycopg2
from urllib.parse import quote_plus
from dotenv import load_dotenv

# GPU ì‚¬ìš© ì„¤ì • (ê°€ëŠ¥í•˜ë©´)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
load_dotenv()

# --- 0. Database Configuration (Supabase PostgreSQL) ---
DB_CFG = {
    "host": os.getenv("SUPABASE_HOST", "aws-1-ap-southeast-1.pooler.supabase.com"),
    "port": os.getenv("SUPABASE_PORT", "5432"),
    "database": os.getenv("SUPABASE_DATABASE", "postgres"),
    "user": os.getenv("SUPABASE_USER", "postgres.wlhignlnfknbsxmbbcno"),
    "password": os.getenv("SUPABASE_PASSWORD"),
    "sslmode": os.getenv("SUPABASE_SSLMODE", "require"),
}

missing_keys = [key for key in ("password",) if not DB_CFG[key]]
if missing_keys:
    raise ValueError(
        "Missing required environment variables: " + ", ".join(f"SUPABASE_{key.upper()}" for key in missing_keys)
    )

DB_CFG["port"] = int(DB_CFG["port"])

# PostgreSQL ì—°ê²° ë¬¸ìì—´ ìƒì„±
encoded_password = quote_plus(DB_CFG['password'])
CONNECTION_STRING = (
    f"postgresql://{DB_CFG['user']}:{encoded_password}@"
    f"{DB_CFG['host']}:{DB_CFG['port']}/{DB_CFG['database']}"
    f"?sslmode={DB_CFG['sslmode']}"
)

# --- DB ì—°ê²° í…ŒìŠ¤íŠ¸ ---
print("\n=== Testing Database Connection ===")
try:
    # psycopg2ë¡œ ì§ì ‘ ì—°ê²° í…ŒìŠ¤íŠ¸
    conn = psycopg2.connect(
        host=DB_CFG['host'],
        port=DB_CFG['port'],
        database=DB_CFG['database'],
        user=DB_CFG['user'],
        password=DB_CFG['password'],
        sslmode=DB_CFG['sslmode']
    )
    cursor = conn.cursor()
    cursor.execute("SELECT version();")
    db_version = cursor.fetchone()
    print(f"âœ… Database connection successful!")
    print(f"   PostgreSQL version: {db_version[0][:50]}...")
    
    # pgvector í™•ì¥ í™•ì¸
    cursor.execute("SELECT * FROM pg_extension WHERE extname = 'vector';")
    if cursor.fetchone():
        print("âœ… pgvector extension is installed.")
    else:
        print("âš ï¸  WARNING: pgvector extension is NOT installed!")
        print("   Please enable it in Supabase Dashboard: Database â†’ Extensions â†’ vector")
    
    cursor.close()
    conn.close()
except Exception as e:
    print(f"âŒ Database connection failed: {e}")
    print("   Please check your DB credentials and network connection.")
    raise

# SQLAlchemy ì—”ì§„ ìƒì„± (PGVector ì´ˆê¸°í™”ìš©)
engine = create_engine(CONNECTION_STRING)
print("Database connection configured.\n")

# --- 1. Load (ë¬¸ì„œ ë¡œë“œ) ---
# ì˜ˆì‹œë¡œ, êµ­ë‚´ ì£¼ì‹ ì‹œì¥ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì›¹ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
# ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” DART ê³µì‹œ, ë‰´ìŠ¤ ê¸°ì‚¬ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.
print("Step 1: Loading documents...")
loader = WebBaseLoader(web_path="https://ko.wikipedia.org/wiki/%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%EC%9D%98_%EC%A3%BC%EC%8B%9D%EC%8B%9C%EC%9E%A5")
documents = loader.load()

# --- 2. Split (ë¬¸ì„œ ë¶„í• ) ---
# ë¬¸ì„œë¥¼ 500ì ë‹¨ìœ„ë¡œ ìë¥´ê³ , 50ìì”© ê²¹ì¹˜ê²Œ í•©ë‹ˆë‹¤.
print("Step 2: Splitting documents...")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
docs = text_splitter.split_documents(documents)
print(f"Total {len(docs)} document chunks created.")

# --- 3. Embed & Store (ì„ë² ë”© ë° ì €ì¥) ---
# [HF ëª¨ë¸ 1: ì„ë² ë”© ëª¨ë¸ (ê²€ìƒ‰ìš©)]
# í•œêµ­ì–´ ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë¸ì„ Hugging Faceì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.
# (ëª¨ë¸ì˜ 'ê°€ì¤‘ì¹˜'ë¥¼ í•™ìŠµí•˜ëŠ” ê²Œ ì•„ë‹ˆë¼, pre-trained ëª¨ë¸ì„ 'ë‹¤ìš´ë¡œë“œ'í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤)
print("Step 3: Loading HF Embedding Model and creating Vector Store...")
model_name_embed = "jhgan/ko-sroberta-multitask" # í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸
model_kwargs = {'device': device}
encode_kwargs = {'normalize_embeddings': True}

hf_embeddings = HuggingFaceEmbeddings(
    model_name=model_name_embed,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

# PostgreSQL (Supabase) ë²¡í„° DBì— ë¶„í• ëœ ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
# ì´ ê³¼ì •ì´ "ì˜¤í”ˆë¶ ì‹œí—˜ì„ ìœ„í•œ ì°¸ê³ ì„œ(Vector DB)"ë¥¼ ë§Œë“œëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
# table_name: ë²¡í„°ë¥¼ ì €ì¥í•  í…Œì´ë¸” ì´ë¦„ (í•„ìš”ì‹œ ë³€ê²½ ê°€ëŠ¥)
print("Creating/Connecting to PostgreSQL Vector Store...")

# ê¸°ì¡´ DBì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì¬ì‚¬ìš©í• ì§€ ê²°ì •
USE_EXISTING_DB = True  # True: ê¸°ì¡´ DB ì‚¬ìš©, False: ìƒˆë¡œ ìƒì„±
COLLECTION_NAME = "langchain_pg_embedding"

if USE_EXISTING_DB:
    try:
        # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ì— ì—°ê²° ì‹œë„
        db = PGVector(
            embeddings=hf_embeddings,
            connection=CONNECTION_STRING,
            collection_name=COLLECTION_NAME,
            use_jsonb=True,
        )
        print(f"âœ… Connected to existing vector store: {COLLECTION_NAME}")
        # ê¸°ì¡´ ë°ì´í„° ê°œìˆ˜ í™•ì¸
        try:
            # ê°„ë‹¨í•œ ê²€ìƒ‰ìœ¼ë¡œ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            test_results = db.similarity_search("test", k=1)
            print(f"   Existing documents in DB: (at least {len(test_results)} found)")
        except:
            print("   (Could not count existing documents)")
    except Exception as e:
        print(f"âš ï¸  Could not connect to existing DB: {e}")
        print("   Creating new vector store...")
        db = PGVector.from_documents(
            documents=docs,
            embedding=hf_embeddings,
            connection=CONNECTION_STRING,
            collection_name=COLLECTION_NAME,
            use_jsonb=True,
        )
        print("âœ… Documents stored in PostgreSQL successfully.")
else:
    # ìƒˆë¡œ ìƒì„±
    db = PGVector.from_documents(
        documents=docs,
        embedding=hf_embeddings,
        connection=CONNECTION_STRING,
        collection_name=COLLECTION_NAME,
        use_jsonb=True,
    )
    print("âœ… Documents stored in PostgreSQL successfully.")

# retrieverë¥¼ ì •ì˜í•©ë‹ˆë‹¤ (ì§ˆë¬¸ì´ ì˜¤ë©´ 3ê°œì˜ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ë„ë¡ ì„¤ì •)
# RAGì˜ "R" (Retrieve) ë‹¨ê³„ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤
retriever = db.as_retriever(search_kwargs={"k": 3})
print("âœ… Retriever configured (will retrieve top 3 relevant documents)")

# --- 4. Retrieve (ê²€ìƒ‰ í…ŒìŠ¤íŠ¸) ---
# "RAG" ì¤‘ "R"ì´ ì˜ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
print("\n" + "="*60)
print("ğŸ” Testing RAG Retrieve Step (ê²€ìƒ‰ í…ŒìŠ¤íŠ¸)")
print("="*60)
query = "í•œêµ­ ì£¼ì‹ì‹œì¥ì˜ íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€?"
retrieved_docs = retriever.invoke(query)
print(f"\nâœ… Retriever Test Succeeded!")
print(f"   Query: {query}")
print(f"   Retrieved {len(retrieved_docs)} documents")
print("\n--- Retrieved Documents Preview ---")
for i, doc in enumerate(retrieved_docs[:3], 1):
    print(f"\n[Document {i}]")
    print(f"{doc.page_content[:200]}...")
    if hasattr(doc, 'metadata') and doc.metadata:
        print(f"Metadata: {doc.metadata}")
print("\n" + "="*60)
print("âœ… RAG Retrieve Step is working correctly!")
print("="*60)

# --- 5. Generate (ìƒì„±) ---
# [HF ëª¨ë¸ 2: ìƒì„±í˜• LLM (ë‹µë³€ìš©)]
# ì‹¤ì œ AI ì—ì´ì „íŠ¸ì˜ 'ë‡Œ' ì—­í• ì„ í•  ëª¨ë¸ì„ Hugging Faceì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.
# âš ï¸ ê²½ê³ : ì´ ëª¨ë¸ì€ VRAMì´ ë§ì´ í•„ìš”í•©ë‹ˆë‹¤. ë¡œì»¬ ì‹¤í–‰ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# (ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œëŠ” GPT, Claude API ë˜ëŠ” Quantized ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸°ë„ í•©ë‹ˆë‹¤)

# ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ í›„ LLM ë¡œë“œ ì‹œë„
SKIP_LLM_LOAD = False  # Trueë¡œ ì„¤ì •í•˜ë©´ LLM ë¡œë“œ ê±´ë„ˆë›°ê¸° (Retrieveë§Œ í…ŒìŠ¤íŠ¸)

if SKIP_LLM_LOAD:
    print("âš ï¸  Skipping LLM loading (SKIP_LLM_LOAD=True)")
    print("   RAG Retrieve step is working. To test full RAG, ensure sufficient disk space.")
else:
    print("Step 5: Loading HF Generator LLM...")

    try:
        model_id = "EleutherAI/polyglot-ko-1.3b" # í•œêµ­ì–´ ì†Œí˜• LLM (ì˜ˆì‹œ)
        # model_id = "gemma-2b" # ë‹¤ë¥¸ ëª¨ë¸ ì˜ˆì‹œ

        tokenizer_llm = AutoTokenizer.from_pretrained(model_id)
        model_llm = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16, # ë©”ëª¨ë¦¬ ì¤„ì´ê¸° (bfloat16ë„ ê°€ëŠ¥)
            low_cpu_mem_usage=True, # CPU ë©”ëª¨ë¦¬ ì ê²Œ ì‚¬ìš©
        ).to(device)

        # LangChainì˜ HuggingFacePipelineìœ¼ë¡œ ê°ì‹¸ê¸°
        # max_new_tokens: ë‹µë³€ ìƒì„± ìµœëŒ€ ê¸¸ì´
        hf_pipeline = pipeline(
            "text-generation",
            model=model_llm,
            tokenizer=tokenizer_llm,
            device=device,
            max_new_tokens=512,
            repetition_penalty=1.1 # ë°˜ë³µ ë°©ì§€
        )

        # LangChainì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ llm ê°ì²´ ìƒì„±
        llm = HuggingFacePipeline(pipeline=hf_pipeline)
        print("âœ… HF Generator LLM loaded.")
    except RuntimeError as e:
        if "No space left on device" in str(e):
            print("âŒ Disk space insufficient for LLM model download.")
            print("   RAG Retrieve step is working correctly.")
            print("   To test full RAG, please free up disk space or use API-based LLM.")
            SKIP_LLM_LOAD = True
            llm = None
        else:
            raise

# --- 6. RAG Chain (ìµœì¢… RAG ì²´ì¸ ìƒì„±) ---
if SKIP_LLM_LOAD or llm is None:
    print("\nâš ï¸  Skipping RAG Chain creation (LLM not loaded)")
    print("   RAG Retrieve step completed successfully!")
    print("   To test full RAG, ensure sufficient disk space for LLM model.")
    print("\n" + "="*60)
    print("âœ… RAG Retrieve Step Test Complete!")
    print("="*60)
    print("\nSummary:")
    print("  âœ… Database connection: Working")
    print("  âœ… Document loading & splitting: Working")
    print("  âœ… Embedding & Vector Store: Working")
    print("  âœ… Retrieve (ê²€ìƒ‰): Working")
    print("  âš ï¸  Generate (ìƒì„±): Skipped (disk space insufficient)")
    print("\nTo enable full RAG:")
    print("  1. Free up disk space (at least 2-3GB needed)")
    print("  2. Or use API-based LLM (OpenAI, Anthropic, etc.)")
    print("="*60)
else:
    # RAG (Retrieval-Augmented Generation) í”„ë¡œì„¸ìŠ¤:
    # 1. Retrieve: ì‚¬ìš©ì ì§ˆë¬¸ì„ ì„ë² ë”©í•˜ì—¬ ë²¡í„° DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    # 2. Augment: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ Contextë¡œ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
    # 3. Generate: LLMì´ Contextì™€ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
    print("Step 6: Creating RAG Chain...")
    
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ (LangChain 1.0 API)
    rag_prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¹ì‹ ì€ ê¸ˆìœµ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ 'Context' ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œë§Œ ì‚¬ìš©ìì˜ 'Question'ì— ëŒ€í•´ ë‹µë³€í•´ ì£¼ì‹­ì‹œì˜¤. Contextì— ì—†ëŠ” ë‚´ìš©ì€ 'ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì‹­ì‹œì˜¤. ì ˆëŒ€ë¡œ ì •ë³´ë¥¼ ì§€ì–´ë‚´ì§€ ë§ˆì‹­ì‹œì˜¤."),
        ("human", "Context:\n{context}\n\nQuestion: {question}\n\nAnswer:")
    ])

    # LangChain 1.0 APIë¥¼ ì‚¬ìš©í•œ RAG ì²´ì¸ ìƒì„±
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    # RAG ì²´ì¸ êµ¬ì„±
    qa_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | rag_prompt
        | llm
        | StrOutputParser()
    )

    print("âœ… RAG Chain created successfully.")

    # --- 7. ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸ (Run & Test) ---
    print("\n" + "="*60)
    print("ğŸš€ RAG Chain Execution Test ğŸš€")
    print("="*60)
    question_to_ask = "í•œêµ­ ì£¼ì‹ì‹œì¥ì—ì„œ ì½”ìŠ¤ë‹¥(KOSDAQ)ì˜ ì—­í• ì€ ë¬´ì—‡ì¸ê°€?"

    # RAG ì²´ì¸ ì‹¤í–‰ ê³¼ì •:
    # 1. Retrieve: 'question_to_ask'ë¥¼ ì„ë² ë”© â†’ ë²¡í„° DBì—ì„œ ìœ ì‚¬ë„ ë†’ì€ ë¬¸ì„œ 3ê°œ ê²€ìƒ‰
    # 2. Augment: ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ Contextë¡œ í”„ë¡¬í”„íŠ¸ì— ì‚½ì…
    # 3. Generate: Context + Questionì´ í¬í•¨ëœ í”„ë¡¬í”„íŠ¸ë¥¼ LLMì— ì „ë‹¬ â†’ ë‹µë³€ ìƒì„±
    print(f"\nğŸ“ Question: {question_to_ask}")
    print("\nâ³ Processing... (This may take a while)")
    # ê²€ìƒ‰ëœ ë¬¸ì„œ ë¨¼ì € ê°€ì ¸ì˜¤ê¸°
    retrieved_docs = retriever.invoke(question_to_ask)

    # RAG ì²´ì¸ ì‹¤í–‰
    answer = qa_chain.invoke(question_to_ask)

    print(f"\nâœ… Answer: {answer}")
    print("\n" + "-"*60)
    print("ğŸ“š Source Documents (ë‹µë³€ì˜ ê·¼ê±° - ê²€ìƒ‰ëœ ë¬¸ì„œë“¤)")
    print("-"*60)
    for i, doc in enumerate(retrieved_docs):
        print(f"\n[Source {i+1}]")
        print(f"{doc.page_content[:200]}...")  # ì²˜ìŒ 200ìë§Œ í‘œì‹œ
        if hasattr(doc, 'metadata'):
            print(f"Metadata: {doc.metadata}")
    print("\n" + "="*60)
    print("âœ… RAG Test Complete!")
    print("="*60)